## conv_filter_visualization.py
**对卷积过程中的卷积核进行可视化，变为图像数据进行输出，可以帮助理解神经网络是如何处理这些图片的，和deep_dream的梯度上升比较相似。这里采用的是VGG16训练好了的参数**
- 模型参数下载地址：https://github.com/fchollet/deep-learning-models/releases
- 下载之后放入"用户根目录/.keras/model"

### 过程
1. 用`model=vgg16.VGG16(weights="imagenet",include_top=Flase)`导入VGG16模型，其中include_top为False，推测top可能是一个全连接层，不导入top可以借用瓶颈层连接全连接层实现迁移学习。另外上面网址下载的参数也可以看出notop相比于完整权重来讲大小会小很多。
2.  用`input_img=model.input`设置一个输入接口，也可以说是placeholder
3.  用`dict([(layer.name, layer) for layer in model.layers[1:]])`创建层的名称与层对象的字典
4.  **代码一开始用layer_name设置了需要可视化的层的名称，可以用3. 创建的字典print所有层的名称，此时用layer_dict[layer_name].output设置输出**
5.  **由于需要对每一个filter进行可视化，所以用了一个for循环，这里采用了`loss = K.mean(layer_output[:, filter_index, :, :])`,注意现在仍然是在创建节点，设置输入为placeholder，然后在layer_name对应的这一层输出，用平均值求得loss**
6.  用`grad=K.gradients(loss, input_img)[0]`根据loss和输出求取更新的梯度，并用设置好的`normalize()`函数将其正则化
7.  用`iterate = K.function([input_img], [loss, grads])`将输入和输出节点连接在一起，**这个步骤代表运算节点已经创建完毕，给定输入[input_img]，会直接得到loss和grads**
8.  用噪声数据创建一定长宽的图片
9.  用`loss_value, grads_value = iterate([input_img_data])`, `input_img_data += grads_value * step`计算梯度并应用梯度到图片中
10.  创建kept_filter数组存储可视化过后的filter图片，每次append((img, loss_value))，img是decode过后的图片，而loss_value是用来排序的
11.  用kept_filters.sort(key=lambda x: x[1], reverse=True)排序，高loss值的在前面，因为**"the filters that have the highest loss are assumed to be better-looking."**
12.  对图片进行排列，输出。

### 笔记
- 对filter进行可视化，并不是直接将它里面存储的权重数据排列一下可视化，而是用了一张噪点图片，对其应用梯度。
- **观察loss可以发现，loss实际上是在不断增大的，因为梯度的应用并不是朝着loss的反向，原因在于loss是自己定义的loss(也许直接将loss设置成一个固定值都可以实现)。其实这和deep_dream非常类似，只不过deep_dream用的是真实图片，这里用的是噪声，实际上都有梯度上升的意思**

### 展望
stitched_filters_8x8.png是所得的filter visualization结果。
![](http://i.imgur.com/gCCF7Y0.jpg)
- 有时候会认为，当我们在黑暗情况下闭上眼睛的时候，看到的是什么，是不是就是像这个例子中得到的上面那幅图的结果一样，将随机噪点作为输出，产生了相当多的filter，然后迭加起来转换成3通道显示在大脑中？
- 但是我们对于**看到的图像**，应当是原封不动的，而**对于图像的理解**，才是真正经过卷积等神经网络处理过后的。**这个疑问也就是说，我们看到的是真正的光，还是经过视神经处理、卷积过后的东西，还是说这两者是合并的？如，我们看到的都是真正光，但意识到的，只能是卷积核处理以及全连接过后的东西**，这个意思也就是说：
  - **我们能够接受世界的光，但是这些光给我们的刺激，都是经过神经网络卷积、连接过后的结果。比如，我们看到一朵花，但是不会真正对花的整个图案有认知或意识，仅仅会只对花的某些特征有意识，当花消失过后，我们并不能够回想起这朵花，而只能够想起它的特征。**
  - **视觉仅仅只是“预览”的作用，并没有进入我们的意识，我们意识到的东西，是经过视神经处理过后传到中枢的东西。这也就很可能出现我们看到东西却没有一点儿反应的现象，因为视神经总是能够预览，但是我们能不能够意识，还是需要靠处理传到中枢，这在神经科学中是可以通过切断某一神经来验证的**
  - 当我们闭上眼睛时，视觉的预览效果是否就不占主导了？当我们揉搓眼睛产生闪光时，这一段的图像是这种“预览”效果吗？
  - **同时，当我们回忆一个物体的时候，也或许是一个生成神经网络，但这网络并不会加载在“预览”当中，否则这种想象就过于强力了。首先从记忆神经网络中读取这个需要回忆的信息，可能是一个hopfield network，之后将这个意识，比如“花朵”作为输入进行转换，转换成图像的刺激，反馈给视神经到达的终端，这样就能够“想象”出一个图案。**
   - 不过这里也有许多复杂的转换，从一朵花真实的光到意识到是一朵花，可能经过了：**1. 视神经感受器——视觉辨别器神经网络——中枢——得到图像的一个印象——语言生成神经网络——得到“花朵”这一文字认知——记忆神经网络**（这里的语言生成神经网络也许是人类进化得出的一种特殊网络，专门用来记忆高频率特征，将其映射成为固定的搭配，如果是低频率特征没有办法转化成语言时，可能记忆时间就较短，另一用途是来进行交流）。当我们进行想象的时候，可能涉及**2. 语言输入——中枢——刺激——视觉生成器神经网络——中枢——视觉判别器神经网络——中枢。这样的结构类似于GAN中的生成-判别网络，因为我们意识到自己的想象，是需要判别网络的，也许我们大多数时刻一直都是在进行生成（即想象），但是判别（意识到自己在想象）的情况较少**
   - 理论上我们很容易出现接受了“预览”而没有意识的情况，但是人们也少有感觉，实际上，当我们在“预览”的时候，存在记忆的暂留（这个可能是用于视神经元的输入），让我们能够完全记住“预览”的内容，但是这个记忆是短暂的，当我们回忆起来一个“预览”时，如果丧失了这种极其短暂的记忆，**我们能够回忆起来的，就是从预览中经过处理得到的特征。比如，看到一张照片，可以暂时记住图片的全貌，但是如果一撤去图片，你几乎就什么都记不住了，而你记住的东西，就是在看照片时经过图像判别网络得到的结果。如果你看图片十分仔细，就能够看到很多特征，这些特征的辨识就调用了神经辨别器网络，传达到中枢神经中，如果这些特征是有“名字”的，就会传达到语言中枢中，形成更强的记忆。**
   - 这里分三种情况，**第一种是短暂记忆，涉及：视神经——短暂记忆网络过程，此时你可以完全想象出图形的全貌，虽然这个时间很短，对于任何图案，都是完全可以记住并回忆起的**
   - **第二种是特征识别记忆，看到图案之后，你会进行特征识别，相当于一种卷积网络，将识别的结果传递给中枢，形成特定的输出（相当于不同位置的神经元输出），此时你能够对其中的特征产生印象，这种记忆很难恢复出你看到的全貌，但是记忆会更深**
   - **第三种最强的记忆，是和一些“强连接”性的网络连接在一起，比如将第二种特征识别的结果和语言中枢、触觉中枢、味觉中枢等合并，这样你能够通过语言等来加深记忆，比如识别出图片中的“花朵、风扇”等，但实际上，能够和这类中枢连接在一起的东西，是少之又少的，但是因为人们记忆深刻的原因，产生了“幸存者效应”，认为这类可表达的存在是偏多的**
- 我们能够从对我们自己的神经网络认知中得到启发。人类研究了很久的机器学习算法，倒不如一个从自然界中借鉴得到的神经网络，大自然确实是一个神奇的存在。
- 然而人类的神经网络只是在视觉方面有胜机器一筹，但是在数据处理和计算方面，机器有很大的优势。 
- **从对自己的认知上，就可以窥见生物神经网络的一貌，至少从我们自己的体会中，我们能够知道，自己的神经网络很可能不是利用所谓的“BP（误差逆传播）”算法进行训练的，因为我们完完全全是非监督学习，我认为我们是先将信息用非监督学习的方式进行分类，然后再将分类结果映射到其他中枢上，映射到语言中枢，就形成了语言的监督学习。比如，我们看到一朵花，首先会用非监督学习记住这朵花，并且看到同类的花，会觉得相似，形成了分类，之后当我们被别人告诉这是“花”，并冠以相应的读音的时候，才真正将语言和实物相连构成监督学习，但此时监督学习的意义可能不如先期的非监督学习大**


