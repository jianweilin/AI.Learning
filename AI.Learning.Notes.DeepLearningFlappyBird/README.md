## deep_q_network.py，根据DQN理解代码
- 神经网络模型的输入是s，即state，shape=(None, 80, 80, 4)，是每次提取的4帧80 x 80的灰度图片。输出是readout，数量为ACTIONS=2的一维数据，**代表进行01操作和10操作分别能够得到的总收益Q。神经网络模型起到了Q-Learning中的Q(S, A)函数，但是这里只有State没有A，神经网络能够学会对应不同的A求得估值函数Q，这就是DQN优势所在。**这个函数同时也以input layer作为return，而不是将input作为形参，是很有趣的建模方式。这个神经网络模型在下文称作估值模型
- cost的计算，是将估值模型得到的readout乘以A得到R，即奖励，A是两个action 01和10，01代表进行跳跃，10代表不进行，这样readout和A相乘就能够得到readout中的相应的一个值了。之后这个值与y求方差，y是采取最高readout所对应操作的R。
- 建立这个flappy bird游戏， 初始输入一个action操作给游戏状态，返回4帧图像。之后检查是否有ckpt，载入ckpt
- feed进入这4帧图像，得到操作的估值readout。接着action的选择有两种，一是根据epsilon概率进行随机操作，二是选择readout估值最大的那个操作
- 将选择的action带入游戏game_state.frame_step中，得到1帧图像，1个奖励float以及1个游戏是否结束的bool，这1帧图像与前面的4帧图像的最后3帧结合，相当于4长度的滑动窗口向右滑1个单位。而这个奖励，如果跳跃过一个管子则+1，没有死亡则+0.1，死亡则-1
- 用一个数组D存储[前面的4帧图像，行动状态，奖励，滑动1过后的图像]，D的存储有上限，达到上限会popleft
- 在训练之前会有一段观察期，观察期会不断更新state，也就是随时间滑动4帧图像，同时会进行可视化，由于使用的是训练好的例子，所以观察期很长，从预览中可以看出训练得非常好，长时间能够避免gameover
- 如果过了观察期会进行训练，首先会随机载入BATCH个D中的样本，由于D中的每一个样本都是[前一时间状态s_j，操作a，奖励r，后一时间状态s_j1]，所以并不需要连续，可以随机采样（也就是马尔科夫状态）。
- 之后进行训练，训练将喂入3个数据。第一个是y_batch，是当前R加上s_j1喂入估值模型得到的readout的最大值的结果。第二个是a_batch，是D中存储的样本的action，第三个是s_batch，是状态s_j。
- 训练是根据上面输入的s_j，用估值模型求取readout(在train_step中是看不到s输入placeholder的，是因为全部已经包含在readout计算节点中了)，然后根据采取的action，得到R，这个R将去和前面喂入的y进行求loss梯度下降。
- 这就相当于是在训练估值网络，喂入s_j和action得到当前的估值，然后y是接下来一个状态的最佳估值，训练网络就是在使其不断趋近于这一最佳估值。训练好了估值网络，就能够根据最大估值来决定接下来的操作。
